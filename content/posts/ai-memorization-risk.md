---
title: "MIT研究：临床AI的隐私风险——模型可能记忆患者数据"
date: 2026-01-31
draft: false
description: "MIT研究人员发现AI模型可能记忆训练数据中的患者隐私信息"
coverImage: "https://images.unsplash.com/photo-1576091160399-112ba8d25d1d?w=800&q=80"
tags: ["AI", "Privacy", "Healthcare", "MIT"]
categories: ["AI"]
---

在数据饥渴算法和网络攻击时代，医学是为数不多的保密仍然至关重要的领域之一。MIT研究人员的一项新研究发现，在去标识化的电子健康记录(EHR)上训练的AI模型可能记忆患者特定信息。

## 记忆风险

在"记忆"中，模型从单个患者记录中提取信息来提供输出，这可能会侵犯患者隐私。论文第一作者、Broad研究所的博士后Sana Tonekaboni表示，高容量模型中的知识可以成为许多社区的资源，但对抗性攻击者可以提示模型提取训练数据的信息。

## 研究方法

研究团队开发了一系列测试来评估隐私风险。他们发现，攻击者对特定患者的信息了解越多，模型就越有可能泄露信息。他们还演示了如何区分模型泛化情况和患者级记忆，以正确评估隐私风险。

论文还强调，一些泄露比其他更有害。例如，模型泄露患者的年龄或人口统计特征可能被归类为更 benign的泄露，而泄露更敏感信息如HIV诊断或酗酒则更为严重。

## 特别脆弱的群体

具有独特病症的患者特别容易受到攻击，因为很容易识别他们。"即使去标识化数据，也取决于你泄露了个人的什么信息，"Tonekaboni说。"一旦你识别了他们，你就知道了更多。"

研究人员计划扩展这项工作，纳入临床医生、隐私专家和法律专家。

"我们的健康数据是私有的，这是有原因的，"Tonekaboni说。"没有理由让其他人知道。"

*来源：[MIT News](https://news.mit.edu/2026/mit-scientists-investigate-memorization-risk-clinical-ai-0105)*
